import time
import base64
import json
from zoneinfo import ZoneInfo
import io
import zipfile
import hashlib
from datetime import datetime, timedelta
import pandas as pd
import requests
from sqlalchemy import text, insert
from sqlalchemy.dialects.postgresql import insert as pg_insert
from dagster import op, get_dagster_logger, DynamicOut, DynamicOutput, In, Out
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from src.db.bronze.models import WbWwwFinReports1d
from src.db.silver.models import WbSales1d, SilverAdjustmentsByProduct, SilverAdjustmentsGeneral, WbLogistics1d


column_type_mapping = {
    "‚Ññ": int,
    "–ù–æ–º–µ—Ä –ø–æ—Å—Ç–∞–≤–∫–∏": int,
    "–ü—Ä–µ–¥–º–µ—Ç": str,
    "–ö–æ–¥ –Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä—ã": int,
    "–ë—Ä–µ–Ω–¥": str,
    "–ê—Ä—Ç–∏–∫—É–ª –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞": str,
    "–ù–∞–∑–≤–∞–Ω–∏–µ": str,
    "–†–∞–∑–º–µ—Ä": str,
    "–ë–∞—Ä–∫–æ–¥": str,
    "–¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞": str,
    "–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ–ø–ª–∞—Ç—ã": str,
    "–î–∞—Ç–∞ –∑–∞–∫–∞–∑–∞ –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–º": "date",
    "–î–∞—Ç–∞ –ø—Ä–æ–¥–∞–∂–∏": "date",
    "–ö–æ–ª-–≤–æ": int,
    "–¶–µ–Ω–∞ —Ä–æ–∑–Ω–∏—á–Ω–∞—è": float,
    "–í–∞–π–ª–¥–±–µ—Ä—Ä–∏–∑ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–ª –¢–æ–≤–∞—Ä (–ü—Ä)": float,
    "–°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–¥—É–∫—Ç–æ–≤—ã–π –¥–∏—Å–∫–æ–Ω—Ç, %": int,
    "–ü—Ä–æ–º–æ–∫–æ–¥, %": str,
    "–ò—Ç–æ–≥–æ–≤–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–∞—è —Å–∫–∏–¥–∫–∞, %": int,
    "–¶–µ–Ω–∞ —Ä–æ–∑–Ω–∏—á–Ω–∞—è —Å —É—á–µ—Ç–æ–º —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–π —Å–∫–∏–¥–∫–∏": float,
    "–†–∞–∑–º–µ—Ä —Å–Ω–∏–∂–µ–Ω–∏—è –∫–í–í –∏–∑-–∑–∞ —Ä–µ–π—Ç–∏–Ω–≥–∞, %": int,
    "–†–∞–∑–º–µ—Ä –∏–∑–º–µ–Ω–µ–Ω–∏—è –∫–í–í –∏–∑-–∑–∞ –∞–∫—Ü–∏–∏, %": float,
    "–°–∫–∏–¥–∫–∞ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–≥–æ –ü–æ–∫—É–ø–∞—Ç–µ–ª—è (–°–ü–ü), %": float,
    "–†–∞–∑–º–µ—Ä –∫–í–í, %": float,
    "–†–∞–∑–º–µ—Ä  –∫–í–í –±–µ–∑ –ù–î–°, % –ë–∞–∑–æ–≤—ã–π": float,
    "–ò—Ç–æ–≥–æ–≤—ã–π –∫–í–í –±–µ–∑ –ù–î–°, %": float,
    "–í–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ —Å –ø—Ä–æ–¥–∞–∂ –¥–æ –≤—ã—á–µ—Ç–∞ —É—Å–ª—É–≥ –ø–æ–≤–µ—Ä–µ–Ω–Ω–æ–≥–æ, –±–µ–∑ –ù–î–°": float,
    "–í–æ–∑–º–µ—â–µ–Ω–∏–µ –∑–∞ –≤—ã–¥–∞—á—É –∏ –≤–æ–∑–≤—Ä–∞—Ç —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∞ –ü–í–ó": float,
    "–≠–∫–≤–∞–π—Ä–∏–Ω–≥/–ö–æ–º–∏—Å—Å–∏–∏ –∑–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é –ø–ª–∞—Ç–µ–∂–µ–π": float,
    "–†–∞–∑–º–µ—Ä –∫–æ–º–∏—Å—Å–∏–∏ –∑–∞ —ç–∫–≤–∞–π—Ä–∏–Ω–≥/–ö–æ–º–∏—Å—Å–∏–∏ –∑–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é –ø–ª–∞—Ç–µ–∂–µ–π, %": float,
    "–¢–∏–ø –ø–ª–∞—Ç–µ–∂–∞ –∑–∞ –≠–∫–≤–∞–π—Ä–∏–Ω–≥/–ö–æ–º–∏—Å—Å–∏–∏ –∑–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é –ø–ª–∞—Ç–µ–∂–µ–π": str,
    "–í–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –í–∞–π–ª–¥–±–µ—Ä—Ä–∏–∑ (–í–í), –±–µ–∑ –ù–î–°": float,
    "–ù–î–° —Å –í–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –í–∞–π–ª–¥–±–µ—Ä—Ä–∏–∑": float,
    "–ö –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏—é –ü—Ä–æ–¥–∞–≤—Ü—É –∑–∞ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –¢–æ–≤–∞—Ä": float,
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ—Å—Ç–∞–≤–æ–∫": int,
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–≤—Ä–∞—Ç–∞": int,
    "–£—Å–ª—É–≥–∏ –ø–æ –¥–æ—Å—Ç–∞–≤–∫–µ —Ç–æ–≤–∞—Ä–∞ –ø–æ–∫—É–ø–∞—Ç–µ–ª—é": float,
    "–î–∞—Ç–∞ –Ω–∞—á–∞–ª–∞ –¥–µ–π—Å—Ç–≤–∏—è —Ñ–∏–∫—Å–∞—Ü–∏–∏": str,
    "–î–∞—Ç–∞ –∫–æ–Ω—Ü–∞ –¥–µ–π—Å—Ç–≤–∏—è —Ñ–∏–∫—Å–∞—Ü–∏–∏": str,
    "–ü—Ä–∏–∑–Ω–∞–∫ —É—Å–ª—É–≥–∏ –ø–ª–∞—Ç–Ω–æ–π –¥–æ—Å—Ç–∞–≤–∫–∏": str,
    "–û–±—â–∞—è —Å—É–º–º–∞ —à—Ç—Ä–∞—Ñ–æ–≤": float,
    "–ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –í–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –í–∞–π–ª–¥–±–µ—Ä—Ä–∏–∑ (–í–í)": float,
    "–í–∏–¥—ã –ª–æ–≥–∏—Å—Ç–∏–∫–∏, —à—Ç—Ä–∞—Ñ–æ–≤ –∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–æ–∫ –í–í": str,
    "–°—Ç–∏–∫–µ—Ä –ú–ü": str,
    "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –±–∞–Ω–∫–∞-—ç–∫–≤–∞–π–µ—Ä–∞": str,
    "–ù–æ–º–µ—Ä –æ—Ñ–∏—Å–∞": str,
    "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –æ—Ñ–∏—Å–∞ –¥–æ—Å—Ç–∞–≤–∫–∏": str,
    "–ò–ù–ù –ø–∞—Ä—Ç–Ω–µ—Ä–∞": str,
    "–ü–∞—Ä—Ç–Ω–µ—Ä": str,
    "–°–∫–ª–∞–¥": str,
    "–°—Ç—Ä–∞–Ω–∞": str,
    "–¢–∏–ø –∫–æ—Ä–æ–±–æ–≤": str,
    "–ù–æ–º–µ—Ä —Ç–∞–º–æ–∂–µ–Ω–Ω–æ–π –¥–µ–∫–ª–∞—Ä–∞—Ü–∏–∏": str,
    "–ù–æ–º–µ—Ä —Å–±–æ—Ä–æ—á–Ω–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è": int,
    "–ö–æ–¥ –º–∞—Ä–∫–∏—Ä–æ–≤–∫–∏": str,
    "–®–ö": int,
    "Srid": str,
    "–í–æ–∑–º–µ—â–µ–Ω–∏–µ –∏–∑–¥–µ—Ä–∂–µ–∫ –ø–æ –ø–µ—Ä–µ–≤–æ–∑–∫–µ/–ø–æ —Å–∫–ª–∞–¥—Å–∫–∏–º –æ–ø–µ—Ä–∞—Ü–∏—è–º —Å —Ç–æ–≤–∞—Ä–æ–º": float,
    "–û—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä –ø–µ—Ä–µ–≤–æ–∑–∫–∏": str,
    "–•—Ä–∞–Ω–µ–Ω–∏–µ": float,
    "–£–¥–µ—Ä–∂–∞–Ω–∏—è": float,
    "–ü–ª–∞—Ç–Ω–∞—è –ø—Ä–∏–µ–º–∫–∞": float,
    "–§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∫–ª–∞–¥–∞ –ø–æ –ø–æ—Å—Ç–∞–≤–∫–µ": float,
    "–ü—Ä–∏–∑–Ω–∞–∫ –ø—Ä–æ–¥–∞–∂–∏ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–º—É –ª–∏—Ü—É": str,
    "–ù–æ–º–µ—Ä –∫–æ—Ä–æ–±–∞ –¥–ª—è –ø–ª–∞—Ç–Ω–æ–π –ø—Ä–∏–µ–º–∫–∏": str,
    "–°–∫–∏–¥–∫–∞ –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–µ —Å–æ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏—è": float,
    "–°–∫–∏–¥–∫–∞ Wibes, %": float,
    "–°—É–º–º–∞ —É–¥–µ—Ä–∂–∞–Ω–Ω–∞—è –∑–∞ –Ω–∞—á–∏—Å–ª–µ–Ω–Ω—ã–µ –±–∞–ª–ª—ã –ø—Ä–æ–≥—Ä–∞–º–º—ã –ª–æ—è–ª—å–Ω–æ—Å—Ç–∏": float,
    "–ö–æ–º–ø–µ–Ω—Å–∞—Ü–∏—è —Å–∫–∏–¥–∫–∏ –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–µ –ª–æ—è–ª—å–Ω–æ—Å—Ç–∏": float,
}


# 1) –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
MSK = ZoneInfo("Europe/Moscow")
SALE_REPORT_LIST_URL   = "https://seller.wildberries.ru/suppliers-mutual-settlements/reports-implementations/reports-daily"
SALE_REPORT_DETAIL_URL = (
    "https://seller.wildberries.ru/"
    "suppliers-mutual-settlements/reports-implementations/"
    "reports-daily/report/{report_id}?isGlobalBalance=false"
)

# 1.1) –ú–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –ø–æ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏—é –¥–ª—è –æ–ø–ª–∞—Ç—ã
PAYMENT_REASON_COL = "–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ–ø–ª–∞—Ç—ã"
REASONS = {
    "sales": {"–ü—Ä–æ–¥–∞–∂–∞", "–í–æ–∑–≤—Ä–∞—Ç"},
    "logistics": {"–õ–æ–≥–∏—Å—Ç–∏–∫–∞"},
    "retentions": {"–£–¥–µ—Ä–∂–∞–Ω–∏–µ"},
    "fines": {"–®—Ç—Ä–∞—Ñ"},
    "compensations": {"–î–æ–±—Ä–æ–≤–æ–ª—å–Ω–∞—è –∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏—è –ø—Ä–∏ –≤–æ–∑–≤—Ä–∞—Ç–µ"},
}

def filter_by_reason(df: pd.DataFrame, reasons: set[str], logger, target_name: str) -> pd.DataFrame:
    if PAYMENT_REASON_COL not in df.columns:
        raise ValueError(f"–ù–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ '{PAYMENT_REASON_COL}' –≤ –∏—Å—Ö–æ–¥–Ω–æ–º DF")
    out = df[df[PAYMENT_REASON_COL].isin(reasons)].copy()
    logger.info(f"[router] {target_name}: {len(out)} —Å—Ç—Ä–æ–∫ –∏–∑ {len(df)}")
    return out


# 2) –°–µ–ª–µ–∫—Ç–æ—Ä—ã
SELECTORS = {
    "open_report_btn":  '//*[@id="app-content-id"]/div[1]/div/div/div/div[3]/div/div[1]/div/div[1]/div[2]/div/div/button',
    "start_date":       '//*[@id="startDate"]',
    "end_date":         '//*[@id="endDate"]',
    "save":             "//button[.//text()[contains(., 'Save') or contains(., '–°–æ—Ö—Ä–∞–Ω–∏—Ç—å')]]",
    "table_wrapper":    "div[class^='Reports-table__wrapper']",
    "row":              "div[class^='Reports-table-row__']",
}


@op(
    ins={"report_date": In(str)},
    out=DynamicOut(),
    required_resource_keys={"selenium_remote"},
    description="–õ–æ–≥–∏–Ω–∏–º—Å—è –≤ –∫–∞–±–∏–Ω–µ—Ç–µ WB, –∑–∞–¥–∞—ë–º –¥–∞—Ç—É –∏ —Å–æ–±–∏—Ä–∞–µ–º report_id + cookies + authv3",
)
def get_report_ids(context, report_date: str):
    logger = get_dagster_logger()
    seen: set[str] = set()
    date_str = datetime.fromisoformat(report_date).strftime("%d.%m.%Y")

    for token_id_str, profile_name in context.resources.selenium_remote.profiles.items():
        api_token_id = int(token_id_str)
        driver = context.resources.selenium_remote(api_token_id)
        try:
            driver.get("https://seller.wildberries.ru/suppliers-mutual-settlements/reports-implementations/reports-daily")
            WebDriverWait(driver, 60).until(
                EC.element_to_be_clickable((By.XPATH, '//*[@id="app-content-id"]/div[1]/div/div/div/div[3]/div/div[1]/div/div[1]/div[2]/div/div/button'))
            ).click()

            start_field = WebDriverWait(driver, 60).until(EC.element_to_be_clickable((By.XPATH, '//*[@id="startDate"]')))
            start_field.clear(); start_field.send_keys(date_str); start_field.send_keys(Keys.TAB); time.sleep(0.5)

            end_field = WebDriverWait(driver, 60).until(EC.element_to_be_clickable((By.XPATH, '//*[@id="endDate"]')))
            end_field.clear(); end_field.send_keys(date_str); end_field.send_keys(Keys.TAB); time.sleep(1)

            WebDriverWait(driver, 15).until(
                EC.element_to_be_clickable((By.XPATH, "//button[.//text()[contains(., 'Save') or contains(., '–°–æ—Ö—Ä–∞–Ω–∏—Ç—å')]]"))
            ).click()

            WebDriverWait(driver, 40).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "div[class^='Reports-table__wrapper']"))
            )

            rows = driver.find_elements(By.CSS_SELECTOR, "div[class^='Reports-table-row__']")
            for row in rows:
                try:
                    btn = row.find_element(By.TAG_NAME, "button")
                    raw = btn.text.strip().replace("\u00A0", "").replace(" ", "")
                    if not raw.isdigit() or len(raw) < 10 or raw in seen:
                        continue
                    seen.add(raw)
                    selenium_cookies = {c["name"]: c["value"] for c in driver.get_cookies()}
                    authv3 = driver.execute_script("return window.localStorage.getItem('wb-eu-passport-v2.access-token');")

                    yield DynamicOutput(
                        {
                            "report_id": raw,
                            "report_date": report_date,   # <‚îÄ‚îÄ –¥–æ–±–∞–≤–∏–ª–∏
                            "legal_entity": profile_name,
                            "cookies": selenium_cookies,
                            "authv3": authv3,
                            "api_token_id": api_token_id, # <‚îÄ‚îÄ –±—ã–ª–æ company_id
                        },
                        mapping_key=raw,
                    )
                except Exception:
                    logger.debug(f"[{profile_name}] —Å—Ç—Ä–æ–∫–∞ –±–µ–∑ –≤–∞–ª–∏–¥–Ω–æ–≥–æ report_id ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
        finally:
            driver.quit()


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# op: —Å–∫–∞—á–∏–≤–∞–µ–º ZIP –∏ –ø–∏—à–µ–º –≤ bronze.wb_www_fin_reports_1d –ø–æ –ù–û–í–û–ô —Å—Ö–µ–º–µ –º–∏–∫—Å–∏–Ω–∞
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
@op(
    ins={"report_meta": In(dict)},
    out=Out(WbWwwFinReports1d),
    required_resource_keys={"postgres"},
    description="–°–∫–∞—á–∏–≤–∞–µ–º ZIP-–∞—Ä—Ö–∏–≤ —á–µ—Ä–µ–∑ HTTP API –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –µ–≥–æ –≤ bronze.wb_www_fin_reports_1d",
)
async def write_fin_report_zip(context, report_meta: dict):
    logger = get_dagster_logger()
    session_maker = context.resources.postgres

    api_token_id = int(report_meta["api_token_id"])
    report_id    = str(report_meta["report_id"])
    report_date  = report_meta.get("report_date")  # ISO (YYYY-MM-DD)
    cookies      = report_meta["cookies"]
    authv3       = report_meta["authv3"]

    url = (
        "https://seller-services.wildberries.ru/ns/reports/seller-wb-balance/api/v1"
        f"/reports/{report_id}/details/archived-excel"
    )

    headers = {
        "accept": "*/*",
        "accept-language": "ru,ru-RU;q=0.9,en-US;q=0.8,en;q=0.7,kk;q=0.6",
        "authorizev3": authv3,
        "X-Supplier-Id": cookies.get("x-supplier-id") or cookies.get("x-supplier-id-external", ""),
        "content-type": "application/json",
        "origin": "https://seller.wildberries.ru",
        "referer": "https://seller.wildberries.ru/",
        "user-agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138 Safari/537.36",
    }

    # ‚îÄ‚îÄ —Å–ª—É–∂–µ–±–Ω—ã–µ —Ç–∞–π–º–∏–Ω–≥–∏ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # –ü–ª–∞–Ω–æ–≤–æ–µ –≤—Ä–µ–º—è –∑–∞–ø—É—Å–∫–∞ –∏–∑ —Ç–µ–≥–∞ Dagster; fallback ‚Äî —Å–µ–π—á–∞—Å
    scheduled_iso = context.dagster_run.tags.get("dagster/scheduled_execution_time")
    run_schedule_dttm = (
        datetime.fromisoformat(scheduled_iso).astimezone(MSK)
        if scheduled_iso else datetime.now(MSK)
    )
    # –§–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –≤—Ä–µ–º—è (–º–æ–º–µ–Ω—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è op)
    run_dttm = datetime.now(MSK)

    business_dttm = run_schedule_dttm.replace(hour=0, minute=0, second=0, microsecond=0) - timedelta(days=1)

    # ‚îÄ‚îÄ –∑–∞–ø—Ä–æ—Å ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    request_dttm = datetime.now(MSK)
    resp = requests.get(url, headers=headers, cookies=cookies, timeout=60)
    response_dttm = datetime.now(MSK)
    resp.raise_for_status()
    receive_dttm = response_dttm

    raw_zip = base64.b64decode(resp.json()["data"]["file"])

    # ‚îÄ‚îÄ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    request_parameters = {"report_id": report_id}
    if report_date:
        request_parameters["report_date"] = report_date

    zip_table = WbWwwFinReports1d.__table__
    stmt = (
        insert(zip_table)
        .values(
            api_token_id=api_token_id,
            run_uuid=context.run_id,
            run_dttm=run_dttm,
            run_schedule_dttm=run_schedule_dttm,
            business_dttm=business_dttm,
            request_dttm=request_dttm,
            request_parameters=request_parameters,
            request_body=url,
            response_code=resp.status_code,
            response_dttm=response_dttm,
            receive_dttm=receive_dttm,
            response_body=raw_zip,
        )
        .returning(*zip_table.c)
    )

    async with session_maker() as sess:
        row = (await sess.execute(stmt)).one()
        await sess.commit()

    obj = WbWwwFinReports1d(**row._mapping)
    logger.info(f"‚úÖ Saved ZIP report {report_id} for token {api_token_id} (business_dttm={business_dttm.isoformat()})")
    return obj

def safe_cast_column(df: pd.DataFrame, col: str, dtype) -> pd.Series:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–∏–ø–∞ –∫–æ–ª–æ–Ω–∫–∏."""
    try:
        if dtype == "date":
            return pd.to_datetime(df[col], errors="coerce").dt.date
        elif dtype in ("Int64", "int"):
            return pd.to_numeric(df[col], errors="coerce").astype("Int64")
        elif dtype == float:
            return pd.to_numeric(df[col], errors="coerce")
        elif dtype == str:
            return df[col].astype(str).fillna("")
        else:
            return df[col].astype(dtype)
    except Exception as e:
        get_dagster_logger().warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫–æ–ª–æ–Ω–∫—É '{col}' –∫ —Ç–∏–ø—É {dtype}: {e}")
        return df[col]


@op(
    ins={"bronze_row": In(WbWwwFinReports1d)},
    out=Out(pd.DataFrame),
    description="–ò–∑–≤–ª–µ–∫–∞–µ—Ç ZIP –∏–∑ –±—Ä–æ–Ω–∑—ã ‚Üí —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ—Ç XLSX ‚Üí –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç DataFrame",
)
def unpack_and_load_excel(context, bronze_row: WbWwwFinReports1d) -> pd.DataFrame:
    logger = get_dagster_logger()

    raw_zip: bytes = bronze_row.response_body
    if not raw_zip:
        raise ValueError(f"–í –±—Ä–æ–Ω–∑–µ –∑–∞–ø–∏—Å—å {bronze_row.request_uuid} –Ω–µ—Ç ZIP-–∞—Ä—Ö–∏–≤–∞")

    # –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º ZIP –∏ —á–∏—Ç–∞–µ–º –ø–µ—Ä–≤—ã–π XLSX-—Ñ–∞–π–ª
    with zipfile.ZipFile(io.BytesIO(raw_zip)) as zf:
        name = zf.namelist()[0]
        logger.debug(f"Unpacking '{name}' from ZIP for request {bronze_row.request_uuid}")
        with zf.open(name) as f:
            df = pd.read_excel(f, engine="openpyxl", dtype={"–ë–∞—Ä–∫–æ–¥": str})

    # –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–∏–ø–æ–≤
    for col, dtype in column_type_mapping.items():
        if col in df.columns:
            df[col] = safe_cast_column(df, col, dtype)

    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º ¬´–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ–ø–ª–∞—Ç—ã¬ª (—Ç—Ä–∏–º –ø—Ä–æ–±–µ–ª–æ–≤)
    if PAYMENT_REASON_COL in df.columns:
        df[PAYMENT_REASON_COL] = df[PAYMENT_REASON_COL].astype(str).str.strip()

    # –ü—Ä–æ–∫–∏–¥—ã–≤–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –ø–æ–ª—è
    df["request_uuid"]  = bronze_row.request_uuid
    df["business_dttm"]  = bronze_row.business_dttm
    df["response_dttm"]  = bronze_row.response_dttm
    df["company_id"] = bronze_row.api_token_id

    logger.info(f"Loaded Excel for request {bronze_row.request_uuid}: shape={df.shape}")
    return df


async def bulk_insert_records(session_maker, table, records: list[dict], chunk_size: int = 500):
    """–í—Å—Ç–∞–≤–∫–∞ –∑–∞–ø–∏—Å–µ–π —á–∞–Ω–∫–∞–º–∏."""
    async with session_maker() as session:
        for i in range(0, len(records), chunk_size):
            chunk = records[i:i + chunk_size]
            stmt = insert(table).values(chunk)
            await session.execute(stmt)
        await session.commit()


async def bulk_upsert_records(session_maker, table, records: list[dict], chunk_size: int = 500):
    """–ò–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–∞—è –≤—Å—Ç–∞–≤–∫–∞: ON CONFLICT DO NOTHING –ø–æ —Å–æ—Å—Ç–∞–≤–Ω–æ–º—É –∫–ª—é—á—É."""
    async with session_maker() as session:
        for i in range(0, len(records), chunk_size):
            chunk = records[i:i + chunk_size]
            stmt = pg_insert(table).values(chunk)
            stmt = stmt.on_conflict_do_nothing()
            await session.execute(stmt)
        await session.commit()


def make_key_hash(row: dict, keys: list[str]) -> str:
    parts = [(row.get(k) or "").strip() for k in keys]
    return hashlib.md5("||".join(parts).encode("utf-8")).hexdigest()


SALES_COLUMN_MAPPING = {
    "–î–∞—Ç–∞ –∑–∞–∫–∞–∑–∞ –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–º":            "order_date",
    "–î–∞—Ç–∞ –ø—Ä–æ–¥–∞–∂–∏":                        "sale_date",
    "–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ–ø–ª–∞—Ç—ã":             "payment_reason",
    "Srid":                               "sr_id",
    "–ù–æ–º–µ—Ä –ø–æ—Å—Ç–∞–≤–∫–∏":                     "income_id",
    "–ë—Ä–µ–Ω–¥":                              "brand",
    "–ù–∞–∑–≤–∞–Ω–∏–µ":                           "name",
    "–ö–æ–¥ –Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä—ã":                   "nm_id",
    "–ê—Ä—Ç–∏–∫—É–ª –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞":                 "supplier_article",
    "–ë–∞—Ä–∫–æ–¥":                             "barcode",
    "–†–∞–∑–º–µ—Ä":                             "tech_size",
    "–ü—Ä–µ–¥–º–µ—Ç":                            "subject",
    "–ö–æ–ª-–≤–æ":                             "quantity",
    "–¶–µ–Ω–∞ —Ä–æ–∑–Ω–∏—á–Ω–∞—è":                     "price",
    "–¶–µ–Ω–∞ —Ä–æ–∑–Ω–∏—á–Ω–∞—è —Å —É—á–µ—Ç–æ–º —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–π —Å–∫–∏–¥–∫–∏": "price_with_discount",
    "–í–∞–π–ª–¥–±–µ—Ä—Ä–∏–∑ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–ª –¢–æ–≤–∞—Ä (–ü—Ä)":  "wb_realization_price",
    "–°–∫–∏–¥–∫–∞ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–≥–æ –ü–æ–∫—É–ø–∞—Ç–µ–ª—è (–°–ü–ü), %":  "spp",
    "–ö –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏—é –ü—Ä–æ–¥–∞–≤—Ü—É –∑–∞ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –¢–æ–≤–∞—Ä": "seller_payout",
    "–†–∞–∑–º–µ—Ä –∫–í–í, %":                      "commision_percent",
    "–≠–∫–≤–∞–π—Ä–∏–Ω–≥/–ö–æ–º–∏—Å—Å–∏–∏ –∑–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é –ø–ª–∞—Ç–µ–∂–µ–π":    "acquiring_amount",
    "–†–∞–∑–º–µ—Ä –∫–æ–º–∏—Å—Å–∏–∏ –∑–∞ —ç–∫–≤–∞–π—Ä–∏–Ω–≥/–ö–æ–º–∏—Å—Å–∏–∏ –∑–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é –ø–ª–∞—Ç–µ–∂–µ–π, %": "acquiring_percent",
    "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –±–∞–Ω–∫–∞-—ç–∫–≤–∞–π–µ—Ä–∞":        "acquiring_bank",
    "–°–∫–ª–∞–¥":                              "warehouse_name",
    "–°—Ç—Ä–∞–Ω–∞":                             "country",
    "–°—É–º–º–∞ —É–¥–µ—Ä–∂–∞–Ω–Ω–∞—è –∑–∞ –Ω–∞—á–∏—Å–ª–µ–Ω–Ω—ã–µ –±–∞–ª–ª—ã –ø—Ä–æ–≥—Ä–∞–º–º—ã –ª–æ—è–ª—å–Ω–æ—Å—Ç–∏": "loyalty_points_withheld_amount"
}


@op(
    ins={"df": In(pd.DataFrame)},
    required_resource_keys={"postgres"},
    description="–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ–¥–∞–∂ –≤ silver.wb_sales_1d"
)
async def load_wb_sales(context, df: pd.DataFrame):
    logger = get_dagster_logger()
    pg = context.resources.postgres
    CHUNK_SIZE = 500

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ ¬´–ü—Ä–æ–¥–∞–∂–∞¬ª/¬´–í–æ–∑–≤—Ä–∞—Ç¬ª
    df = filter_by_reason(df, REASONS["sales"], logger, "wb_sales_1d")
    if df.empty:
        logger.info("wb_sales_1d: –ø—É—Å—Ç–æ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
        return

    # 1) –ø—Ä–æ–≤–µ—Ä—è–µ–º –∏ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º
    expected_index = ["business_dttm", "request_uuid", "response_dttm", "company_id"]
    missing = [c for c in expected_index if c not in df.columns]
    if missing:
        raise ValueError(f"–í incoming df –Ω–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫: {missing}")

    df_renamed = df.rename(columns=SALES_COLUMN_MAPPING)

    # 2) –∫–æ–Ω–≤–µ—Ä—Ç–∏–º –¥–∞—Ç—ã —É–∂–µ –≤ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–Ω–æ–º df
    for col in ("order_date", "sale_date"):
        if col in df_renamed.columns:
            df_renamed[col] = pd.to_datetime(df_renamed[col], errors="coerce").dt.date

    # 3) –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ —Ü–µ–ª–µ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –µ—Å—Ç—å
    target_cols = expected_index + list(SALES_COLUMN_MAPPING.values())
    missing = [c for c in target_cols if c not in df_renamed.columns]
    if missing:
        raise ValueError(f"–ü–æ—Å–ª–µ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è –Ω–µ –Ω–∞—à–ª–æ—Å—å –∫–æ–ª–æ–Ω–æ–∫: {missing}")

    # 4) –ª–æ–≥–∏—Ä—É–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    dupe_key = [
         "business_dttm", "sr_id", "payment_reason"
    ]
    dupes = df_renamed[df_renamed.duplicated(subset=dupe_key, keep=False)]
    if not dupes.empty:
        logger.warning(f"üö® –ù–∞–π–¥–µ–Ω–æ {len(dupes)} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ sales –ø–æ —Å–æ—Å—Ç–∞–≤–Ω–æ–º—É –∫–ª—é—á—É")

    # 5) –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏ –≤—Å—Ç–∞–≤–∫–∞
    records = df_renamed[target_cols].to_dict(orient="records")
    logger.info(f"–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –≤ wb_sales_1d: {len(records)}")
    await bulk_upsert_records(pg, WbSales1d.__table__, records, chunk_size=CHUNK_SIZE)

    logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ wb_sales_1d –∑–∞–≤–µ—Ä—à–µ–Ω–∞")


LOGISTICS_COLUMN_MAPPING = {
    "–î–∞—Ç–∞ –∑–∞–∫–∞–∑–∞ –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–º":              "order_date",
    "–î–∞—Ç–∞ –ø—Ä–æ–¥–∞–∂–∏":                         "sale_date",
    "–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ–ø–ª–∞—Ç—ã":               "payment_reason",
    "Srid":                                 "sr_id",
    "–ë—Ä–µ–Ω–¥":                                "brand",
    "–ù–∞–∑–≤–∞–Ω–∏–µ":                             "name",
    "–ö–æ–¥ –Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä—ã":                     "nm_id",
    "–ê—Ä—Ç–∏–∫—É–ª –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞":                   "supplier_article",
    "–ë–∞—Ä–∫–æ–¥":                               "barcode",
    "–†–∞–∑–º–µ—Ä":                               "tech_size",
    "–ù–æ–º–µ—Ä –ø–æ—Å—Ç–∞–≤–∫–∏":                       "income_id",
    "–°–∫–ª–∞–¥":                                "warehouse_name",
    "–°—Ç—Ä–∞–Ω–∞":                               "country",
    "–£—Å–ª—É–≥–∏ –ø–æ –¥–æ—Å—Ç–∞–≤–∫–µ —Ç–æ–≤–∞—Ä–∞ –ø–æ–∫—É–ø–∞—Ç–µ–ª—é": "logistics_cost",
    "–í–∏–¥—ã –ª–æ–≥–∏—Å—Ç–∏–∫–∏, —à—Ç—Ä–∞—Ñ–æ–≤ –∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–æ–∫ –í–í": "logistic_type",
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ—Å—Ç–∞–≤–æ–∫":                  "delivery_quantity",
}


@op(
    ins={"df": In(pd.DataFrame)},
    required_resource_keys={"postgres"},
    description="–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø–æ –ª–æ–≥–∏—Å—Ç–∏–∫–µ –≤ silver.wb_logistics_1d"
)
async def load_wb_logistics(context, df: pd.DataFrame):
    logger = get_dagster_logger()
    pg = context.resources.postgres
    CHUNK_SIZE = 500

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ ¬´–õ–æ–≥–∏—Å—Ç–∏–∫–∞¬ª
    df = filter_by_reason(df, REASONS["logistics"], logger, "wb_logistics_1d")
    if df.empty:
        logger.info("wb_logistics_1d: –ø—É—Å—Ç–æ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
        return

    # 1) –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (–∏–∑ –±—Ä–æ–Ω–∑—ã)
    expected_index = ["business_dttm", "company_id", "request_uuid", "response_dttm"]
    missing = [c for c in expected_index if c not in df.columns]
    if missing:
        raise ValueError(f"–í incoming df –Ω–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫: {missing}")

    # 2) –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –≤ –º–æ–¥–µ–ª—å–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
    df2 = df.rename(columns=LOGISTICS_COLUMN_MAPPING)

    # 3) –ø—Ä–∏–≤–æ–¥–∏–º –¢–û–õ–¨–ö–û –ø–æ–ª–µ–≤—ã–µ –¥–∞—Ç—ã; business_dttm/response_dttm –Ω–µ —Ç—Ä–æ–≥–∞–µ–º (–æ–Ω–∏ TIMESTAMPTZ)
    for col in ("order_date", "sale_date"):
        if col in df2.columns:
            df2[col] = pd.to_datetime(df2[col], errors="coerce").dt.date

    # 4) –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ —Ü–µ–ª–µ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç
    target_cols = list(dict.fromkeys(expected_index + list(LOGISTICS_COLUMN_MAPPING.values())))
    missing = [c for c in target_cols if c not in df2.columns]
    if missing:
        raise ValueError(f"–ü–æ—Å–ª–µ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∫–æ–ª–æ–Ω–æ–∫: {missing}")

    # 5) –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—Ä—É–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –∫–ª—é—á—É (PK):
    dupe_key = ["business_dttm", "sr_id", "logistic_type"]
    dupes = df2[df2.duplicated(subset=dupe_key, keep=False)]
    if not dupes.empty:
        logger.warning(f"üö® –ù–∞–π–¥–µ–Ω–æ {len(dupes)} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ –ª–æ–≥–∏—Å—Ç–∏–∫–µ –ø–æ –∫–ª—é—á—É {dupe_key}")

    # 6) –≤—Å—Ç–∞–≤–∫–∞
    records = df2[target_cols].to_dict(orient="records")
    logger.info(f"–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –≤ wb_logistics_1d: {len(records)}")
    await bulk_upsert_records(pg, WbLogistics1d.__table__, records, chunk_size=CHUNK_SIZE)

    logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ wb_logistics_1d –∑–∞–≤–µ—Ä—à–µ–Ω–∞")


def _to_str(v):
    import pandas as pd
    if pd.isna(v):
        return None
    if isinstance(v, float) and v.is_integer():
        return str(int(v))
    return str(v)


ADJUSTMENTS_ALLOWED_REASONS = {
    "–®—Ç—Ä–∞—Ñ",
    "–£–¥–µ—Ä–∂–∞–Ω–∏–µ",
    "–î–æ–±—Ä–æ–≤–æ–ª—å–Ω–∞—è –∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏—è –ø—Ä–∏ –≤–æ–∑–≤—Ä–∞—Ç–µ",
    "–ö–æ–º–ø–µ–Ω—Å–∞—Ü–∏—è —É—â–µ—Ä–±–∞",
    "–ö–æ–º–ø–µ–Ω—Å–∞—Ü–∏—è —Å–∫–∏–¥–∫–∏ –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–µ –ª–æ—è–ª—å–Ω–æ—Å—Ç–∏",
    "–°—É–º–º–∞ —É–¥–µ—Ä–∂–∞–Ω–Ω–∞—è –∑–∞ –Ω–∞—á–∏—Å–ª–µ–Ω–Ω—ã–µ –±–∞–ª–ª—ã –ø—Ä–æ–≥—Ä–∞–º–º—ã –ª–æ—è–ª—å–Ω–æ—Å—Ç–∏",
    "–°—Ç–æ–∏–º–æ—Å—Ç—å —É—á–∞—Å—Ç–∏—è –≤ –ø—Ä–æ–≥—Ä–∞–º–º–µ –ª–æ—è–ª—å–Ω–æ—Å—Ç–∏",
}

ADJ_BY_PRODUCT_COLUMN_MAPPING = {
    "–ù–æ–º–µ—Ä –ø–æ—Å—Ç–∞–≤–∫–∏": "income_id",
    "–ü—Ä–µ–¥–º–µ—Ç": "subject",
    "–ö–æ–¥ –Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä—ã": "nomenclature_code",
    "–ë—Ä–µ–Ω–¥": "brand",
    "–ê—Ä—Ç–∏–∫—É–ª –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞": "supplier_article",
    "–ù–∞–∑–≤–∞–Ω–∏–µ": "name",
    "–†–∞–∑–º–µ—Ä": "tech_size",
    "–ë–∞—Ä–∫–æ–¥": "barcode",
    "–¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞": "doc_type_name",
    "–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ–ø–ª–∞—Ç—ã": "supplier_oper_name",
    "–î–∞—Ç–∞ –∑–∞–∫–∞–∑–∞ –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–º": "order_date",
    "–î–∞—Ç–∞ –ø—Ä–æ–¥–∞–∂–∏": "sale_date",
    "–û–±—â–∞—è —Å—É–º–º–∞ —à—Ç—Ä–∞—Ñ–æ–≤": "penalty",
    "–í–∏–¥—ã –ª–æ–≥–∏—Å—Ç–∏–∫–∏, —à—Ç—Ä–∞—Ñ–æ–≤ –∏ –¥–æ–ø–ª–∞—Ç": "bonus_type_name",
    "–ù–æ–º–µ—Ä –æ—Ñ–∏—Å–∞": "office_number",
    "–°–∫–ª–∞–¥": "warehouse_name",
    "–°—Ç—Ä–∞–Ω–∞": "country",
    "–¢–∏–ø –∫–æ—Ä–æ–±–æ–≤": "box_type",
    "–®–ö": "shk_id",
    "Srid": "sr_id",
    "–ö –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏—é –ü—Ä–æ–¥–∞–≤—Ü—É –∑–∞ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –¢–æ–≤–∞—Ä": "seller_payout",
    "–ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –í–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –í–∞–π–ª–¥–±–µ—Ä—Ä–∏–∑ (–í–í)": "additional_payment",
    "–°—É–º–º–∞ —É–¥–µ—Ä–∂–∞–Ω–Ω–∞—è –∑–∞ –Ω–∞—á–∏—Å–ª–µ–Ω–Ω—ã–µ –±–∞–ª–ª—ã –ø—Ä–æ–≥—Ä–∞–º–º—ã –ª–æ—è–ª—å–Ω–æ—Å—Ç–∏": "cashback_amount",
    "–ö–æ–º–ø–µ–Ω—Å–∞—Ü–∏—è —Å–∫–∏–¥–∫–∏ –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–µ –ª–æ—è–ª—å–Ω–æ—Å—Ç–∏": "cashback_discount",
}


@op(
    ins={"df": In(pd.DataFrame)},
    required_resource_keys={"postgres"},
    description="–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç —É–¥–µ—Ä–∂–∞–Ω–∏—è/–∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏–∏ –ø–æ —Ç–æ–≤–∞—Ä–∞–º –≤ silver.adjustments_by_product"
)
async def load_wb_adjustments_by_product(context, df: pd.DataFrame):
    logger = get_dagster_logger()
    pg = context.resources.postgres
    CHUNK_SIZE = 500

    # 0) —Ñ–∏–ª—å—Ç—Ä –ø–æ –¥–æ–ø—É—Å—Ç–∏–º—ã–º –ø—Ä–∏—á–∏–Ω–∞–º
    df = filter_by_reason(df, ADJUSTMENTS_ALLOWED_REASONS, logger, "adjustments_by_product")
    if df.empty:
        logger.info("adjustments_by_product: –ø—É—Å—Ç–æ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
        return

    # 1) –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    expected_index = ["business_dttm", "company_id", "request_uuid", "response_dttm"]
    missing = [c for c in expected_index if c not in df.columns]
    if missing:
        raise ValueError(f"–í incoming df –Ω–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫: {missing}")

    # 2) –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫ –ø–æ –º–∞–ø–ø–∏–Ω–≥—É
    df1 = df.rename(columns=ADJ_BY_PRODUCT_COLUMN_MAPPING).copy()

    # 3) –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –ø–æ–ª–µ–π
    # –¥–∞—Ç—ã
    for col in ("order_date", "sale_date"):
        if col in df1.columns:
            df1[col] = safe_cast_column(df1, col, "date")
    # —á–∏—Å–ª–æ–≤—ã–µ (–¥–µ–Ω–µ–∂–Ω—ã–µ)
    for col in ("penalty", "seller_payout", "additional_payment", "cashback_amount", "cashback_discount"):
        if col in df1.columns:
            df1[col] = safe_cast_column(df1, col, float)
    # —Å—Ç—Ä–æ–∫–æ–≤—ã–µ ¬´–Ω–æ–º–µ—Ä–∞ –æ—Ñ–∏—Å–∞¬ª –∏ –ø—Ä–æ—á–µ–µ
    if "office_number" in df1.columns:
        df1["office_number"] = df1["office_number"].astype(str)
    for col in ("sr_id", "shk_id", "barcode", "supplier_article", "office_number"):
        if col in df1.columns:
            df1[col] = df1[col].map(_to_str)
    # 4) —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä –ø–æ–ª–µ–π –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏
    target_cols = [
        # —Å–ª—É–∂–µ–±–Ω—ã–µ
        "business_dttm", "sr_id", "supplier_oper_name", "company_id", "request_uuid", "response_dttm",
        # –¥–∞–Ω–Ω—ã–µ
        "income_id", "subject", "nomenclature_code", "brand", "supplier_article", "name", "tech_size",
        "barcode", "doc_type_name", "order_date", "sale_date", "penalty", "bonus_type_name", "office_number",
        "warehouse_name", "country", "box_type", "shk_id", "seller_payout", "additional_payment",
        "cashback_amount", "cashback_discount",
    ]
    for col in target_cols:
        if col not in df1.columns:
            df1[col] = None

    # 5) –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –¥—É–±–ª–µ–π –ø–æ PK
    pk_key = ["business_dttm", "sr_id", "supplier_oper_name"]
    dupes = df1[df1.duplicated(subset=pk_key, keep=False)]
    if not dupes.empty:
        logger.warning(f"üö® –ù–∞–π–¥–µ–Ω–æ {len(dupes)} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ adjustments_by_product –ø–æ –∫–ª—é—á—É {pk_key}")

    # 6) –≤—Å—Ç–∞–≤–∫–∞
    records = df1[target_cols].to_dict(orient="records")
    logger.info(f"–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –≤ silver.adjustments_by_product: {len(records)}")
    await bulk_upsert_records(pg, SilverAdjustmentsByProduct.__table__, records, chunk_size=CHUNK_SIZE)
    logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ adjustments_by_product –∑–∞–≤–µ—Ä—à–µ–Ω–∞")


ADJ_GENERAL_COLUMN_MAPPING = {
    "–ù–æ–º–µ—Ä –ø–æ—Å—Ç–∞–≤–∫–∏": "income_id",
    "–ü—Ä–µ–¥–º–µ—Ç": "subject",
    "–ö–æ–¥ –Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä—ã": "nm_id",
    "–ë—Ä–µ–Ω–¥": "brand",
    "–ê—Ä—Ç–∏–∫—É–ª –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞": "supplier_article",
    "–ù–∞–∑–≤–∞–Ω–∏–µ": "name",
    "–†–∞–∑–º–µ—Ä": "tech_size",
    "–ë–∞—Ä–∫–æ–¥": "barcode",
    "–¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞": "doc_type_name",
    "–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ–ø–ª–∞—Ç—ã": "supplier_oper_name",
    "–î–∞—Ç–∞ –∑–∞–∫–∞–∑–∞ –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–º": "order_date",
    "–î–∞—Ç–∞ –ø—Ä–æ–¥–∞–∂–∏": "sale_date",
    "–û–±—â–∞—è —Å—É–º–º–∞ —à—Ç—Ä–∞—Ñ–æ–≤": "penalty",
    "–í–∏–¥—ã –ª–æ–≥–∏—Å—Ç–∏–∫–∏, —à—Ç—Ä–∞—Ñ–æ–≤ –∏ –¥–æ–ø–ª–∞—Ç": "bonus_type_name",
    "–ù–æ–º–µ—Ä –æ—Ñ–∏—Å–∞": "office_number",
    "–°–∫–ª–∞–¥": "warehouse_name",
    "–°—Ç—Ä–∞–Ω–∞": "country",
    "–¢–∏–ø –∫–æ—Ä–æ–±–æ–≤": "box_type",
    "–®–ö": "shk_id",
    "Srid": "sr_id",
    "–£–¥–µ—Ä–∂–∞–Ω–∏—è": "deduction",
}


@op(
    ins={"df": In(pd.DataFrame)},
    required_resource_keys={"postgres"},
    description="–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç —É–¥–µ—Ä–∂–∞–Ω–∏—è/–∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏–∏ –æ–±—â–µ–≥–æ —Ç–∏–ø–∞ –≤ silver.adjustments_general"
)
async def load_wb_adjustments_general(context, df: pd.DataFrame):
    logger = get_dagster_logger()
    pg = context.resources.postgres
    CHUNK_SIZE = 500

    # 0) —Ñ–∏–ª—å—Ç—Ä –ø–æ –¥–æ–ø—É—Å—Ç–∏–º—ã–º –ø—Ä–∏—á–∏–Ω–∞–º
    df = filter_by_reason(df, ADJUSTMENTS_ALLOWED_REASONS, logger, "adjustments_general")
    if df.empty:
        logger.info("adjustments_general: –ø—É—Å—Ç–æ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
        return

    # 1) –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    expected_index = ["business_dttm", "company_id", "request_uuid", "response_dttm"]
    missing = [c for c in expected_index if c not in df.columns]
    if missing:
        raise ValueError(f"–í incoming df –Ω–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫: {missing}")

    # 2) –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ
    df1 = df.rename(columns=ADJ_GENERAL_COLUMN_MAPPING).copy()

    # 3) –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–∏–ø–æ–≤
    for col in ("order_date", "sale_date"):
        if col in df1.columns:
            df1[col] = safe_cast_column(df1, col, "date")
    for col in ("penalty", "deduction"):
        if col in df1.columns:
            df1[col] = safe_cast_column(df1, col, float)
    if "office_number" in df1.columns:
        df1["office_number"] = df1["office_number"].astype(str)
    for col in ("sr_id", "shk_id", "barcode", "supplier_article", "office_number"):
        if col in df1.columns:
            df1[col] = df1[col].map(_to_str)
    # 4) —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä –ø–æ–ª–µ–π
    target_cols = [
        # —Å–ª—É–∂–µ–±–Ω—ã–µ
        "business_dttm", "sr_id", "supplier_oper_name", "company_id", "request_uuid", "response_dttm",
        # –¥–∞–Ω–Ω—ã–µ
        "income_id", "subject", "nm_id", "brand", "supplier_article", "name", "tech_size", "barcode",
        "doc_type_name", "order_date", "sale_date", "penalty", "bonus_type_name", "office_number",
        "warehouse_name", "country", "box_type", "shk_id", "deduction",
    ]
    for col in target_cols:
        if col not in df1.columns:
            df1[col] = None

    # 5) –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –¥—É–±–ª–µ–π –ø–æ PK
    pk_key = ["business_dttm", "sr_id", "supplier_oper_name"]
    dupes = df1[df1.duplicated(subset=pk_key, keep=False)]
    if not dupes.empty:
        logger.warning(f"üö® –ù–∞–π–¥–µ–Ω–æ {len(dupes)} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ adjustments_general –ø–æ –∫–ª—é—á—É {pk_key}")

    # 6) –≤—Å—Ç–∞–≤–∫–∞
    records = df1[target_cols].to_dict(orient="records")
    logger.info(f"–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –≤ silver.adjustments_general: {len(records)}")
    await bulk_upsert_records(pg, SilverAdjustmentsGeneral.__table__, records, chunk_size=CHUNK_SIZE)
    logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ adjustments_general –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
